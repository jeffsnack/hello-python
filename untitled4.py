# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dGsfo0XsTZI0Cg5rfvyP2HAtyDga_mDm
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
from keras.preprocessing.image import ImageDataGenerator
from keras.utils.np_utils import *
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import numpy as np
import matplotlib.pyplot as plt
import random
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D
from keras import optimizers
#資料前處理
def get_img(directory):
    Images = []
    Labels = []
    label = 0
    for labels in os.listdir(directory): #將所有資料分類
        if labels == 'cardboard':
            label = 0
        elif labels == 'glass':
            label = 1
        elif labels == 'metal':
            label = 2
        elif labels == 'paper':
            label = 3
        elif labels == 'plastic':
            label = 4
        elif labels == 'trash':
            label = 5
         
        for image_file in os.listdir(directory+r'/'+labels): #讀取所有回收物的照片
            image = cv2.imread(directory+r'/'+labels+r'/'+image_file)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = cv2.resize(image,(32,32))
            Images.append(image)
            Labels.append(label)
 
    return shuffle(Images,Labels,random_state = 200)
            
            
            
    
def get_classlabel(class_code):
    labels = {0:'cardboard',1:'glass',2:'metal',3:'paper',4:'plastic',5:'trash'}
    return labels[class_code]

Images,Labels = get_img(r'/content/drive/MyDrive/garbage classification/Garbage classification')

#資料正規化
Images = np.array(Images)
Labels = np.array(Labels)

(trainData, testData, trainLabels, testLabels) = train_test_split(Images,Labels, test_size=0.15, random_state=42)
(trainData, valiData, trainLabels, valiLabels) = train_test_split(trainData, trainLabels, test_size=0.2, random_state=42)

trainData_normalize = trainData.astype('float32') / 255.0
testData_normalize = testData.astype('float32') / 255.0
valiData_normalize = valiData.astype('float32') / 255.0


trainLabels_hot = to_categorical(trainLabels)
testLabels_hot = to_categorical(testLabels)
valiLabels_hot = to_categorical(valiLabels)



#資料擴增
train_datagen = ImageDataGenerator(

    zca_whitening=False,

    rotation_range=40,

    width_shift_range=0.2,

    height_shift_range=0.2,

    shear_range=0.2,

    zoom_range=0.2,

    horizontal_flip=True,

    fill_mode='nearest')

train_datagen.fit(trainData_normalize)


#組建CNN
model = Sequential()

model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu',input_shape=(32,32,3)))
model.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))
model.add(MaxPooling2D(3,3))
model.add(Conv2D(filters=256,kernel_size=(3,3),activation='relu'))
#model.add(Conv2D(filters=1024,kernel_size=(3,3),activation='relu'))
#model.add(Conv2D(filters=100,kernel_size=(3,3),activation='relu'))
#model.add(Conv2D(filters=50,kernel_size=(3,3),activation='relu'))
model.add(MaxPooling2D(3,3))
model.add(Flatten())
model.add(Dense(512,activation='relu'))
model.add(Dense(1024,activation='relu'))
model.add(Dropout(rate=0.5))
model.add(Dense(6,activation='softmax'))


model.compile(optimizer=optimizers.Adam(lr=0.0001),loss = 'categorical_crossentropy',metrics=['accuracy'])

print(model.summary())



epochs = 15
batch_size = 64
steps_per_epoch=int((len(trainData)*2)/batch_size)

trained = model.fit_generator(train_datagen.flow(trainData_normalize,trainLabels_hot,
               batch_size=batch_size),steps_per_epoch=steps_per_epoch,epochs = epochs,
               validation_data=(valiData_normalize, valiLabels_hot))
#trained = model.fit_generator(train_datagen.flow(Images,Labels,batch_size = 64)
#,steps_per_epoch=round(len(Images)/64),epochs=15,validation_data =())

def show_train_history(train_acc,test_acc):
    plt.plot(trained.history[train_acc])
    plt.plot(trained.history[test_acc])
    plt.title('Train History')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

def show_train_history_loss(train_loss,test_loss):
    plt.plot(trained.history[train_loss])
    plt.plot(trained.history[test_loss])
    plt.title('Train History')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

show_train_history('accuracy','val_accuracy')
show_train_history_loss('loss','val_loss')

